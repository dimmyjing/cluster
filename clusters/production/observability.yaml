# yaml-language-server: $schema=https://raw.githubusercontent.com/yannh/kubernetes-json-schema/refs/heads/master/v1.31.1/namespace-v1.json
apiVersion: v1
kind: Namespace
metadata:
  name: observability
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/refs/heads/main/helmrepository-source-v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: clickstack-repo
  namespace: observability
spec:
  interval: 1m0s
  url: https://hyperdxio.github.io/helm-charts
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/refs/heads/main/helmrelease-helm-v2.json
# https://artifacthub.io/packages/helm/hdx-oss-v2/clickstack
# https://github.com/hyperdxio/helm-charts/blob/main/charts/clickstack/values.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: clickstack-release
  namespace: observability
spec:
  interval: 10m
  chart:
    spec:
      chart: clickstack
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: clickstack-repo
      version: 1.0.0
  values:
    global:
      storageClassName: hcloud-volumes
    hyperdx:
      env:
        - name: BETA_CH_OTEL_JSON_SCHEMA_ENABLED
          value: "true"
      useExistingConfigSecret: true
      existingConfigSecret: hyperdx-external-config
      existingConfigConnectionsKey: connections.json
      existingConfigSourcesKey: sources.json
      otelExporterEndpoint: "http://otel-collector.observability.svc.cluster.local:4318"
      apiKey: 48daf6aa-a32e-4c36-afbb-4b5ffb5ec27f
    clickhouse:
      enabled: false
    otel:
      clickhouseEndpoint: "http://clickhouse-cluster01.clickhouse.svc.cluster.local:8123"
      clickhousePrometheusEndpoint: "clickhouse-operator-altinity-clickhouse-operator-metrics.clickhouse.svc.cluster.local:8888"
      clickhouseDatabase: otel
      clickhouseUser: otel-collector
      env:
        - name: CLICKHOUSE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: clickhouse-opentelemetry-collector-account
              key: password
        - name: OTEL_AGENT_FEATURE_GATE_ARG
          value: "--feature-gates=clickhouse.json"
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/refs/heads/main/helmrepository-source-v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: opentelemetry-repo
  namespace: observability
spec:
  interval: 1m0s
  url: https://open-telemetry.github.io/opentelemetry-helm-charts
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/refs/heads/main/helmrelease-helm-v2.json
# https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-operator
# https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-operator/values.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: opentelemetry-operator-release
  namespace: observability
spec:
  interval: 10m
  chart:
    spec:
      chart: opentelemetry-operator
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: opentelemetry-repo
      version: 0.99.2
  values:
    manager:
      operator:
        targetallocator:
          mtls: true
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opamp-bridge
rules:
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors
    verbs: ["*"]
  - apiGroups:
      - ""
    resources:
      - pods
    verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opamp-bridge
subjects:
  - kind: ServiceAccount
    name: opamp-bridge-opamp-bridge
    namespace: observability
roleRef:
  kind: ClusterRole
  name: opamp-bridge
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpAMPBridge
metadata:
  name: opamp-bridge
  namespace: observability
spec:
  endpoint: "http://clickstack-release-app.observability.svc.cluster.local:4320"
  capabilities:
    AcceptsRemoteConfig: true
    ReportsEffectiveConfig: true
    ReportsHealth: true
    ReportsRemoteConfig: true
    ReportsOwnLogs: true
    ReportsOwnMetrics: true
    ReportsOwnTraces: true
  componentsAllowed:
    extensions:
      - bearertokenauth/hyperdx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opentelemetry-targetallocator
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/metrics
      - services
      - endpoints
      - pods
      - namespaces
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
    verbs: ["get"]
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs: ["get", "list", "watch"]
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
  - nonResourceURLs: ["/apis/*"]
    verbs: ["get"]
  - nonResourceURLs: ["/apis"]
    verbs: ["get"]
  - nonResourceURLs: ["/api/*"]
    verbs: ["get"]
  - nonResourceURLs: ["/api"]
    verbs: ["get"]
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
      - podmonitors
    verbs:
      - "*"
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - certificaterequests
      - certificates
    verbs: ["create", "get", "list", "watch", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opentelemetry-targetallocator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-targetallocator
subjects:
  - kind: ServiceAccount
    name: agent-collector-targetallocator
    namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opentelemetry-agent-collector
rules:
  - apiGroups: [""]
    resources:
      - pods
      - namespaces
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources:
      - replicasets
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources:
      - replicasets
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - nodes/stats
    verbs: ["get", "watch", "list"]
  - apiGroups: [""]
    resources:
      - pods
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opentelemetry-agent-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-agent-collector
subjects:
  - kind: ServiceAccount
    name: agent-collector-collector
    namespace: observability
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: agent-collector
  namespace: observability
  labels:
    opentelemetry.io/opamp-managed: opamp-bridge
spec:
  mode: daemonset
  env:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_NODE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
  targetAllocator:
    enabled: true
    allocationStrategy: per-node
    prometheusCR:
      enabled: true
      podMonitorSelector: {}
      serviceMonitorSelector: {}
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: otel-collector
              scrape_interval: 10s
              static_configs:
                - targets:
                    - 0.0.0.0:8888
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 20s
        endpoint: ${env:K8S_NODE_IP}:10250
      receiver_creator/logs:
        discovery:
          default_annotations:
            io.opentelemetry.discovery.logs/enabled: "true"
          enabled: true
        watch_observers:
          - k8s_observer
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      k8s_observer:
        auth_type: serviceAccount
        node: ${env:K8S_NODE_NAME}
    processors:
      batch: {}
      k8sattributes:
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
            - service.namespace
            - service.name
            - service.version
            - service.instance.id
          otel_annotations: true
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
    exporters:
      otlp:
        endpoint: clickstack-release-otel-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true
    service:
      extensions:
        - health_check
        - k8s_observer
      pipelines:
        traces:
          receivers: [otlp]
          processors: [k8sattributes, memory_limiter, batch]
          exporters: [otlp]
        metrics:
          receivers: [otlp, prometheus, kubeletstats]
          processors: [k8sattributes, memory_limiter, batch]
          exporters: [otlp]
        logs:
          receivers: [otlp, receiver_creator/logs]
          processors: [k8sattributes, memory_limiter, batch]
          exporters: [otlp]
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: ${env:MY_POD_IP}
                    port: 8888
# ---
# TODO: go autoinstrumentation (https://opentelemetry.io/docs/platforms/kubernetes/operator/automatic/#go)
# apiVersion: opentelemetry.io/v1alpha1
# kind: Instrumentation
# metadata:
#   name: otel-instrumentation
# spec:
#   exporter:
#     endpoint: http://agent-collector.observability.svc.cluster.local:4317
#   propagators:
#     - tracecontext
#     - baggage
#     - b3
#   go:
#     env:
#       # Required if endpoint is set to 4317.
#       # Go autoinstrumentation uses http/proto by default
#       # so data must be sent to 4318 instead of 4317.
#       - name: OTEL_EXPORTER_OTLP_ENDPOINT
#         value: http://agent-collector.observability.svc.cluster.local:4318
